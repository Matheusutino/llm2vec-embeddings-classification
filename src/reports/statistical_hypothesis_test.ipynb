{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matheus/Desktop/Itens/Projetos/llm2vec-embeddings-classification\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.core.utils import read_json\n",
    "\n",
    "# Configura o Pandas para exibir todas as colunas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def load_results_to_dataframe(base_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load results from JSON files into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Traverse the directory structure\n",
    "    for dataset_name in os.listdir(base_path):\n",
    "        dataset_path = os.path.join(base_path, dataset_name)\n",
    "        if os.path.isdir(dataset_path):\n",
    "            for model_type in os.listdir(dataset_path):\n",
    "                model_type_path = os.path.join(dataset_path, model_type)\n",
    "                \n",
    "                if os.path.isdir(model_type_path):\n",
    "                    for model_name in os.listdir(model_type_path):\n",
    "                        model_name_path = os.path.join(model_type_path, model_name)\n",
    "                        \n",
    "                        # Define paths based on whether prompt_name is needed\n",
    "                        if model_type != \"bert\":\n",
    "                            subdirs = [os.path.join(model_name_path, prompt) for prompt in os.listdir(model_name_path)]\n",
    "                        else:\n",
    "                            subdirs = [model_name_path]\n",
    "                        \n",
    "                        # Process results.json files from determined paths\n",
    "                        for subdir in subdirs:\n",
    "                            for classifier in os.listdir(subdir):\n",
    "                                classifier_path = os.path.join(subdir, classifier)\n",
    "                                \n",
    "                                # Check for the results.json in the classifier path\n",
    "                                json_file_path = os.path.join(classifier_path, 'results.json')\n",
    "                                \n",
    "                                if os.path.isfile(json_file_path):\n",
    "                                    result_data = read_json(json_file_path)\n",
    "\n",
    "                                    keys_to_extract = [\"split0_test_f1_score\", \"split1_test_f1_score\", \"split2_test_f1_score\", \"split3_test_f1_score\", \"split4_test_f1_score\", \"mean_test_f1_score\", \"embedding_generation_time\", \"embedding_generation_size\"]\n",
    "    \n",
    "                                    # Extrai apenas as chaves especificadas\n",
    "                                    result_data= {key: result_data.get(key) for key in keys_to_extract}\n",
    "                                    \n",
    "                                    # Add metadata to the result data\n",
    "                                    result_data['dataset_name'] = dataset_name\n",
    "                                    result_data['model_type'] = model_type\n",
    "                                    result_data['model_name'] = model_name\n",
    "                                    result_data['classifier'] = classifier\n",
    "                                    \n",
    "                                    # Add prompt_name if applicable\n",
    "                                    if model_type != \"bert\":\n",
    "                                        result_data['prompt_name'] = os.path.basename(subdir)\n",
    "                                    else:\n",
    "                                        result_data['prompt_name'] = None\n",
    "                                    \n",
    "                                    results.append(result_data)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Specify the order of the columns\n",
    "    columns_first = ['dataset_name', 'model_type', 'model_name', 'classifier']\n",
    "    if 'prompt_name' in results_df.columns:\n",
    "        columns_first.append('prompt_name')\n",
    "    column_order = columns_first + [col for col in results_df.columns if col not in columns_first]\n",
    "    results_df = results_df[column_order]\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>classifier</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>split0_test_f1_score</th>\n",
       "      <th>split1_test_f1_score</th>\n",
       "      <th>split2_test_f1_score</th>\n",
       "      <th>split3_test_f1_score</th>\n",
       "      <th>split4_test_f1_score</th>\n",
       "      <th>mean_test_f1_score</th>\n",
       "      <th>embedding_generation_time</th>\n",
       "      <th>embedding_generation_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dmoz-Science.csv</td>\n",
       "      <td>bert</td>\n",
       "      <td>sentence-transformers_all-distilroberta-v1</td>\n",
       "      <td>knn</td>\n",
       "      <td>None</td>\n",
       "      <td>0.781488</td>\n",
       "      <td>0.781423</td>\n",
       "      <td>0.807855</td>\n",
       "      <td>0.787449</td>\n",
       "      <td>0.798073</td>\n",
       "      <td>0.791258</td>\n",
       "      <td>4.378122</td>\n",
       "      <td>18432128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dmoz-Science.csv</td>\n",
       "      <td>bert</td>\n",
       "      <td>sentence-transformers_all-MiniLM-L12-v2</td>\n",
       "      <td>knn</td>\n",
       "      <td>None</td>\n",
       "      <td>0.793738</td>\n",
       "      <td>0.784877</td>\n",
       "      <td>0.813567</td>\n",
       "      <td>0.790159</td>\n",
       "      <td>0.797160</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>2.609955</td>\n",
       "      <td>9216128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dmoz-Science.csv</td>\n",
       "      <td>bert</td>\n",
       "      <td>sentence-transformers_all-MiniLM-L6-v2</td>\n",
       "      <td>knn</td>\n",
       "      <td>None</td>\n",
       "      <td>0.790149</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.819735</td>\n",
       "      <td>0.787873</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.798932</td>\n",
       "      <td>2.709429</td>\n",
       "      <td>9216128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dmoz-Science.csv</td>\n",
       "      <td>bert</td>\n",
       "      <td>sentence-transformers_all-mpnet-base-v2</td>\n",
       "      <td>knn</td>\n",
       "      <td>None</td>\n",
       "      <td>0.821972</td>\n",
       "      <td>0.812738</td>\n",
       "      <td>0.834973</td>\n",
       "      <td>0.811677</td>\n",
       "      <td>0.830549</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>3.978258</td>\n",
       "      <td>18432128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dmoz-Science.csv</td>\n",
       "      <td>llm2vec</td>\n",
       "      <td>McGill-NLP_LLM2Vec-Meta-Llama-3-8B-Instruct-mn...</td>\n",
       "      <td>knn</td>\n",
       "      <td>instruction_summary_prompt</td>\n",
       "      <td>0.802117</td>\n",
       "      <td>0.798172</td>\n",
       "      <td>0.828751</td>\n",
       "      <td>0.798943</td>\n",
       "      <td>0.820523</td>\n",
       "      <td>0.809701</td>\n",
       "      <td>88.551751</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>review_polarity.csv</td>\n",
       "      <td>llm2vec</td>\n",
       "      <td>McGill-NLP_LLM2Vec-Mistral-7B-Instruct-v2-mntp...</td>\n",
       "      <td>knn</td>\n",
       "      <td>instruction_classification_prompt</td>\n",
       "      <td>0.795752</td>\n",
       "      <td>0.779712</td>\n",
       "      <td>0.746347</td>\n",
       "      <td>0.781351</td>\n",
       "      <td>0.768741</td>\n",
       "      <td>0.774381</td>\n",
       "      <td>158.887130</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>review_polarity.csv</td>\n",
       "      <td>llm2vec</td>\n",
       "      <td>McGill-NLP_LLM2Vec-Mistral-7B-Instruct-v2-mntp...</td>\n",
       "      <td>knn</td>\n",
       "      <td>base_prompt</td>\n",
       "      <td>0.785244</td>\n",
       "      <td>0.779712</td>\n",
       "      <td>0.748410</td>\n",
       "      <td>0.781009</td>\n",
       "      <td>0.749738</td>\n",
       "      <td>0.768823</td>\n",
       "      <td>147.025850</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>review_polarity.csv</td>\n",
       "      <td>llm2vec</td>\n",
       "      <td>McGill-NLP_LLM2Vec-Sheared-LLaMA-mntp-supervised</td>\n",
       "      <td>knn</td>\n",
       "      <td>instruction_summary_prompt</td>\n",
       "      <td>0.763671</td>\n",
       "      <td>0.759978</td>\n",
       "      <td>0.729923</td>\n",
       "      <td>0.760221</td>\n",
       "      <td>0.709838</td>\n",
       "      <td>0.744726</td>\n",
       "      <td>50.659078</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>review_polarity.csv</td>\n",
       "      <td>llm2vec</td>\n",
       "      <td>McGill-NLP_LLM2Vec-Sheared-LLaMA-mntp-supervised</td>\n",
       "      <td>knn</td>\n",
       "      <td>instruction_classification_prompt</td>\n",
       "      <td>0.744483</td>\n",
       "      <td>0.747043</td>\n",
       "      <td>0.736201</td>\n",
       "      <td>0.751192</td>\n",
       "      <td>0.723348</td>\n",
       "      <td>0.740454</td>\n",
       "      <td>50.584882</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>review_polarity.csv</td>\n",
       "      <td>llm2vec</td>\n",
       "      <td>McGill-NLP_LLM2Vec-Sheared-LLaMA-mntp-supervised</td>\n",
       "      <td>knn</td>\n",
       "      <td>base_prompt</td>\n",
       "      <td>0.746662</td>\n",
       "      <td>0.733876</td>\n",
       "      <td>0.754925</td>\n",
       "      <td>0.761963</td>\n",
       "      <td>0.721314</td>\n",
       "      <td>0.743748</td>\n",
       "      <td>47.369127</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dataset_name model_type  \\\n",
       "0       Dmoz-Science.csv       bert   \n",
       "1       Dmoz-Science.csv       bert   \n",
       "2       Dmoz-Science.csv       bert   \n",
       "3       Dmoz-Science.csv       bert   \n",
       "4       Dmoz-Science.csv    llm2vec   \n",
       "..                   ...        ...   \n",
       "105  review_polarity.csv    llm2vec   \n",
       "106  review_polarity.csv    llm2vec   \n",
       "107  review_polarity.csv    llm2vec   \n",
       "108  review_polarity.csv    llm2vec   \n",
       "109  review_polarity.csv    llm2vec   \n",
       "\n",
       "                                            model_name classifier  \\\n",
       "0           sentence-transformers_all-distilroberta-v1        knn   \n",
       "1              sentence-transformers_all-MiniLM-L12-v2        knn   \n",
       "2               sentence-transformers_all-MiniLM-L6-v2        knn   \n",
       "3              sentence-transformers_all-mpnet-base-v2        knn   \n",
       "4    McGill-NLP_LLM2Vec-Meta-Llama-3-8B-Instruct-mn...        knn   \n",
       "..                                                 ...        ...   \n",
       "105  McGill-NLP_LLM2Vec-Mistral-7B-Instruct-v2-mntp...        knn   \n",
       "106  McGill-NLP_LLM2Vec-Mistral-7B-Instruct-v2-mntp...        knn   \n",
       "107   McGill-NLP_LLM2Vec-Sheared-LLaMA-mntp-supervised        knn   \n",
       "108   McGill-NLP_LLM2Vec-Sheared-LLaMA-mntp-supervised        knn   \n",
       "109   McGill-NLP_LLM2Vec-Sheared-LLaMA-mntp-supervised        knn   \n",
       "\n",
       "                           prompt_name  split0_test_f1_score  \\\n",
       "0                                 None              0.781488   \n",
       "1                                 None              0.793738   \n",
       "2                                 None              0.790149   \n",
       "3                                 None              0.821972   \n",
       "4           instruction_summary_prompt              0.802117   \n",
       "..                                 ...                   ...   \n",
       "105  instruction_classification_prompt              0.795752   \n",
       "106                        base_prompt              0.785244   \n",
       "107         instruction_summary_prompt              0.763671   \n",
       "108  instruction_classification_prompt              0.744483   \n",
       "109                        base_prompt              0.746662   \n",
       "\n",
       "     split1_test_f1_score  split2_test_f1_score  split3_test_f1_score  \\\n",
       "0                0.781423              0.807855              0.787449   \n",
       "1                0.784877              0.813567              0.790159   \n",
       "2                0.799200              0.819735              0.787873   \n",
       "3                0.812738              0.834973              0.811677   \n",
       "4                0.798172              0.828751              0.798943   \n",
       "..                    ...                   ...                   ...   \n",
       "105              0.779712              0.746347              0.781351   \n",
       "106              0.779712              0.748410              0.781009   \n",
       "107              0.759978              0.729923              0.760221   \n",
       "108              0.747043              0.736201              0.751192   \n",
       "109              0.733876              0.754925              0.761963   \n",
       "\n",
       "     split4_test_f1_score  mean_test_f1_score  embedding_generation_time  \\\n",
       "0                0.798073            0.791258                   4.378122   \n",
       "1                0.797160            0.795900                   2.609955   \n",
       "2                0.797702            0.798932                   2.709429   \n",
       "3                0.830549            0.822382                   3.978258   \n",
       "4                0.820523            0.809701                  88.551751   \n",
       "..                    ...                 ...                        ...   \n",
       "105              0.768741            0.774381                 158.887130   \n",
       "106              0.749738            0.768823                 147.025850   \n",
       "107              0.709838            0.744726                  50.659078   \n",
       "108              0.723348            0.740454                  50.584882   \n",
       "109              0.721314            0.743748                  47.369127   \n",
       "\n",
       "     embedding_generation_size  \n",
       "0                     18432128  \n",
       "1                      9216128  \n",
       "2                      9216128  \n",
       "3                     18432128  \n",
       "4                          128  \n",
       "..                         ...  \n",
       "105                        128  \n",
       "106                        128  \n",
       "107                        128  \n",
       "108                        128  \n",
       "109                        128  \n",
       "\n",
       "[110 rows x 13 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = 'results' \n",
    "\n",
    "df = load_results_to_dataframe(base_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert</td>\n",
       "      <td>[0.7814881599520914, 0.7814233885535803, 0.807...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llm2vec</td>\n",
       "      <td>[0.8021170140394833, 0.7981716507412795, 0.828...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type                                                  0\n",
       "0       bert  [0.7814881599520914, 0.7814233885535803, 0.807...\n",
       "1    llm2vec  [0.8021170140394833, 0.7981716507412795, 0.828..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Defina os splits que você quer combinar\n",
    "splits = ['split0_test_f1_score', \n",
    "          'split1_test_f1_score', \n",
    "          'split2_test_f1_score', \n",
    "          'split3_test_f1_score', \n",
    "          'split4_test_f1_score']\n",
    "\n",
    "# Agrupar pelo 'prompt_name' e combinar os splits em uma lista para cada 'prompt_name'\n",
    "df_combined = df.groupby('model_type')[splits].apply(lambda x: x.values.flatten().tolist()).reset_index()\n",
    "\n",
    "# Exibir o resultado\n",
    "df_combined  # A coluna 0 contém a lista dos splits combinados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = df_combined[df_combined['model_type'] == \"bert\"][0].tolist()[0]\n",
    "llm2vec = df_combined[df_combined['model_type'] == \"llm2vec\"][0].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste de Shapiro-Wilk para BERT: Estatística = 0.9267336662178758, p-valor = 3.263592024901407e-05\n",
      "Teste de Shapiro-Wilk para LLM2Vec: Estatística = 0.9744571322962526, p-valor = 4.350122302301677e-07\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Se o p-valor for maior que 0,05: Os dados não apresentam evidências suficientes para rejeitar a hipótese de normalidade, ou seja, os dados podem ser considerados normalmente distribuídos.\n",
    "#Se o p-valor for menor que 0,05: Os dados provavelmente não seguem uma distribuição normal, o que pode indicar a necessidade de ajustes ou o uso de testes não paramétricos.\n",
    "\n",
    "# Teste de Shapiro-Wilk para normalidade\n",
    "shapiro_bert_stat, shapiro_bert_p = stats.shapiro(bert)\n",
    "shapiro_llm2vec_stat, shapiro_llm2vec_p = stats.shapiro(llm2vec)\n",
    "\n",
    "# Imprimir os resultados do teste de Shapiro-Wilk\n",
    "print(f'Teste de Shapiro-Wilk para BERT: Estatística = {shapiro_bert_stat}, p-valor = {shapiro_bert_p}')\n",
    "print(f'Teste de Shapiro-Wilk para LLM2Vec: Estatística = {shapiro_llm2vec_stat}, p-valor = {shapiro_llm2vec_p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatística de Kruskal-Wallis: 8.349214935625344\n",
      "Valor-p: 0.003858540119644899\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# Executando o teste de Kruskal-Wallis\n",
    "stat, p_value = kruskal(bert, llm2vec)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(f\"Estatística de Kruskal-Wallis: {stat}\")\n",
    "print(f\"Valor-p: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediana BERT: 0.7890110456523249\n",
      "Mediana LLM2Vec: 0.7987677932179514\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "import numpy as np\n",
    "\n",
    "# Cálculo das medianas para BERT e LLM2Vec\n",
    "mediana_bert = np.median(bert)\n",
    "mediana_llm2vec = np.median(llm2vec)\n",
    "\n",
    "print(f'Mediana BERT: {mediana_bert}')\n",
    "print(f'Mediana LLM2Vec: {mediana_llm2vec}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base_prompt</td>\n",
       "      <td>[0.8041448892389337, 0.8074692058976428, 0.847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>instruction_classification_prompt</td>\n",
       "      <td>[0.7991820588375381, 0.7924220543974968, 0.830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instruction_summary_prompt</td>\n",
       "      <td>[0.8021170140394833, 0.7981716507412795, 0.828...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         prompt_name  \\\n",
       "0                        base_prompt   \n",
       "1  instruction_classification_prompt   \n",
       "2         instruction_summary_prompt   \n",
       "\n",
       "                                                   0  \n",
       "0  [0.8041448892389337, 0.8074692058976428, 0.847...  \n",
       "1  [0.7991820588375381, 0.7924220543974968, 0.830...  \n",
       "2  [0.8021170140394833, 0.7981716507412795, 0.828...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Defina os splits que você quer combinar\n",
    "splits = ['split0_test_f1_score', \n",
    "          'split1_test_f1_score', \n",
    "          'split2_test_f1_score', \n",
    "          'split3_test_f1_score', \n",
    "          'split4_test_f1_score']\n",
    "\n",
    "# Agrupar pelo 'prompt_name' e combinar os splits em uma lista para cada 'prompt_name'\n",
    "df_combined = df.groupby('prompt_name')[splits].apply(lambda x: x.values.flatten().tolist()).reset_index()\n",
    "\n",
    "# Exibir o resultado\n",
    "df_combined  # A coluna 0 contém a lista dos splits combinados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = df_combined[df_combined['prompt_name'] == \"base_prompt\"][0].tolist()[0]\n",
    "instruction_summary_prompt = df_combined[df_combined['prompt_name'] == \"instruction_summary_prompt\"][0].tolist()[0]\n",
    "instruction_classification_prompt = df_combined[df_combined['prompt_name'] == \"instruction_classification_prompt\"][0].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk:\n",
      "base_prompt: ShapiroResult(statistic=np.float64(0.9780683841149544), pvalue=np.float64(0.016842197234145814))\n",
      "instruction_summary_prompt: ShapiroResult(statistic=np.float64(0.9658230116257912), pvalue=np.float64(0.0008753770107978666))\n",
      "instruction_classification_prompt: ShapiroResult(statistic=np.float64(0.9668374884912305), pvalue=np.float64(0.0011021261106809728))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "#Se o p-valor for maior que 0,05: Os dados não apresentam evidências suficientes para rejeitar a hipótese de normalidade, ou seja, os dados podem ser considerados normalmente distribuídos.\n",
    "#Se o p-valor for menor que 0,05: Os dados provavelmente não seguem uma distribuição normal, o que pode indicar a necessidade de ajustes ou o uso de testes não paramétricos.\n",
    "\n",
    "# Teste de Shapiro-Wilk\n",
    "shapiro_base = shapiro(base_prompt)\n",
    "shapiro_summary = shapiro(instruction_summary_prompt)\n",
    "shapiro_classification = shapiro(instruction_classification_prompt)\n",
    "\n",
    "print(\"Shapiro-Wilk:\")\n",
    "print(\"base_prompt:\", shapiro_base)\n",
    "print(\"instruction_summary_prompt:\", shapiro_summary)\n",
    "print(\"instruction_classification_prompt:\", shapiro_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatística de Kruskal-Wallis: 1.0434580723085982\n",
      "Valor-p: 0.5934934886155303\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# Executando o teste de Kruskal-Wallis\n",
    "stat, p_value = kruskal(base_prompt, instruction_summary_prompt, instruction_classification_prompt)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(f\"Estatística de Kruskal-Wallis: {stat}\")\n",
    "print(f\"Valor-p: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = df[df['model_type'] == 'bert']['embedding_generation_time']\n",
    "llm2vec = df[df['model_type'] == 'llm2vec']['embedding_generation_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste de Shapiro-Wilk para BERT: Estatística = 0.6772026339835802, p-valor = 2.0822588366427486e-05\n",
      "Teste de Shapiro-Wilk para LLM2Vec: Estatística = 0.7480545454568027, p-valor = 3.9981888472766954e-11\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Se o p-valor for maior que 0,05: Os dados não apresentam evidências suficientes para rejeitar a hipótese de normalidade, ou seja, os dados podem ser considerados normalmente distribuídos.\n",
    "#Se o p-valor for menor que 0,05: Os dados provavelmente não seguem uma distribuição normal, o que pode indicar a necessidade de ajustes ou o uso de testes não paramétricos.\n",
    "\n",
    "# Teste de Shapiro-Wilk para normalidade\n",
    "shapiro_bert_stat, shapiro_bert_p = stats.shapiro(bert)\n",
    "shapiro_llm2vec_stat, shapiro_llm2vec_p = stats.shapiro(llm2vec)\n",
    "\n",
    "# Imprimir os resultados do teste de Shapiro-Wilk\n",
    "print(f'Teste de Shapiro-Wilk para BERT: Estatística = {shapiro_bert_stat}, p-valor = {shapiro_bert_p}')\n",
    "print(f'Teste de Shapiro-Wilk para LLM2Vec: Estatística = {shapiro_llm2vec_stat}, p-valor = {shapiro_llm2vec_p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatística de Kruskal-Wallis: 47.89483483483485\n",
      "Valor-p: 4.4970478603666605e-12\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# Executando o teste de Kruskal-Wallis\n",
    "stat, p_value = kruskal(bert, llm2vec)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(f\"Estatística de Kruskal-Wallis: {stat}\")\n",
    "print(f\"Valor-p: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediana BERT: 4.344724655151367\n",
      "Mediana LLM2Vec: 135.5660218000412\n",
      "O modelo BERT tem os menores valores de tempo de geração de embeddings.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "import numpy as np\n",
    "\n",
    "# Cálculo das medianas para BERT e LLM2Vec\n",
    "mediana_bert = np.median(bert)\n",
    "mediana_llm2vec = np.median(llm2vec)\n",
    "\n",
    "print(f'Mediana BERT: {mediana_bert}')\n",
    "print(f'Mediana LLM2Vec: {mediana_llm2vec}')\n",
    "\n",
    "# Comparando as medianas\n",
    "if mediana_bert < mediana_llm2vec:\n",
    "    print(\"O modelo BERT tem os menores valores de tempo de geração de embeddings.\")\n",
    "else:\n",
    "    print(\"O modelo LLM2Vec tem os menores valores de tempo de geração de embeddings.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
